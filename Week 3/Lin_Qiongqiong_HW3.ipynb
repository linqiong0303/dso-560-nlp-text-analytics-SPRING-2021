{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pattern\n",
    "from pattern.text.en import singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews:pd.DataFrame=pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\",encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Used regex cleaning and substitution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AH_PATTERN = re.compile(r'\\bA{1,20}H{1,20}\\b', flags=re.IGNORECASE)\n",
    "uh_PATTERN = re.compile(r'\\bU{1,20}H{1,20}\\b', flags=re.IGNORECASE)\n",
    "ew_PATTERN = re.compile(r'\\bE{1,20}W{1,20}\\w+\\b', flags=re.IGNORECASE)\n",
    "Time_pattern=re.compile(r'\\b(\\d+(?::\\d+)?(?:/\\d+)?(?::\\d+)?[\\s]?(?:A|P)M)\\b', flags=re.IGNORECASE)\n",
    "noise_number=re.compile(r'(\\$?\\d+(?:\\w+)?)')\n",
    "underscore=re.compile(r'(\\b.?_\\w+?)')\n",
    "ugh_PATTERN = re.compile(r'\\bU{1,20}G{1,20}h{0,20}\\b', flags=re.IGNORECASE)\n",
    "slow_PATTERN = re.compile(r'\\bS{1,20}L{1,20}O{1,20}W{1,20}\\b', flags=re.IGNORECASE)\n",
    "too_PATTERN = re.compile(r'\\bT{1,20}O{1,20}\\b', flags=re.IGNORECASE)\n",
    "so_PATTERN = re.compile(r'\\bS{1,20}O{1,20}\\b', flags=re.IGNORECASE)\n",
    "yo_PATTERN = re.compile(r'\\by(?:e|o)(?:a|p)[h]?\\b', flags=re.IGNORECASE)\n",
    "all_PATTERN = re.compile(r'\\bA{1,20}L{1,20}\\b', flags=re.IGNORECASE)\n",
    "top_PATTERN = re.compile(r'\\btop[s]?(?:ped|ping)?\\b', flags=re.IGNORECASE)\n",
    "still_PATTERN = re.compile(r'\\bstil{1,20}\\b', flags=re.IGNORECASE)\n",
    "wait_PATTERN = re.compile(r'\\bwait(?:ing|ed)?\\b', flags=re.IGNORECASE)\n",
    "spill_PATTERN = re.compile(r'\\bspill(?:ing|ed)?\\b', flags=re.IGNORECASE)\n",
    "gr_PATTERN = re.compile(r'\\bG{1,20}R{1,20}\\b', flags=re.IGNORECASE)\n",
    "spend_PATTERN = re.compile(r'\\bspen(?:t|ding)?\\b', flags=re.IGNORECASE)\n",
    "why_PATTERN = re.compile(r'\\bwhy{1,20}\\b', flags=re.IGNORECASE)\n",
    "way_PATTERN = re.compile(r'\\bway{1,20}\\b', flags=re.IGNORECASE)\n",
    "woo_PATTERN = re.compile(r'\\bwo{1,20}\\b', flags=re.IGNORECASE)\n",
    "grill_PATTERN = re.compile(r'\\bgrill(?:ed|ing)?\\b', flags=re.IGNORECASE)\n",
    "worry_PATTERN = re.compile(r'\\bworr(?:y|ied)?\\b', flags=re.IGNORECASE)\n",
    "work_PATTERN = re.compile(r'\\bwork(?:ed|ing)?\\b', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['review'] = reviews.review.str.replace(AH_PATTERN, '_ahhh_token_')\n",
    "reviews['review'] = reviews.review.str.replace(uh_PATTERN, 'uh')\n",
    "reviews['review'] = reviews.review.str.replace(Time_pattern, 'time')\n",
    "reviews['review'] = reviews.review.str.replace(noise_number, 'number')\n",
    "reviews['review'] = reviews.review.str.replace(underscore, 'underscore')\n",
    "reviews['review'] = reviews.review.str.replace(ugh_PATTERN, 'ugh')\n",
    "reviews['review'] = reviews.review.str.replace('wikipedium', 'wikipedia')\n",
    "reviews['review'] = reviews.review.str.replace('yum', 'yummy')\n",
    "reviews['review'] = reviews.review.str.replace(slow_PATTERN, 'slow')\n",
    "reviews['review'] = reviews.review.str.replace(ew_PATTERN, 'ew')\n",
    "reviews['review'] = reviews.review.str.replace(too_PATTERN, 'too')\n",
    "reviews['review'] = reviews.review.str.replace(so_PATTERN, 'so')\n",
    "reviews['review'] = reviews.review.str.replace(yo_PATTERN, 'y')\n",
    "reviews['review'] = reviews.review.str.replace(all_PATTERN, 'all')\n",
    "reviews['review'] = reviews.review.str.replace(top_PATTERN, 'top')\n",
    "reviews['review'] = reviews.review.str.replace(still_PATTERN, 'still')\n",
    "reviews['review'] = reviews.review.str.replace(wait_PATTERN, 'wait')\n",
    "reviews['review'] = reviews.review.str.replace('thorugh', 'thorough')\n",
    "reviews['review'] = reviews.review.str.replace('thrown', 'throw')\n",
    "reviews['review'] = reviews.review.str.replace(spill_PATTERN, 'spill')\n",
    "reviews['review'] = reviews.review.str.replace('tastele', 'tasteless')\n",
    "reviews['review'] = reviews.review.str.replace('tasteles', 'tasteless')\n",
    "reviews['review'] = reviews.review.str.replace(gr_PATTERN, 'gr')\n",
    "reviews['review'] = reviews.review.str.replace('tastey', 'tasty')\n",
    "reviews['review'] = reviews.review.str.replace('tasted', 'taste')\n",
    "reviews['review'] = reviews.review.str.replace(spend_PATTERN, 'spend')\n",
    "reviews['review'] = reviews.review.str.replace(why_PATTERN, 'why')\n",
    "reviews['review'] = reviews.review.str.replace(way_PATTERN, 'way')\n",
    "reviews['review'] = reviews.review.str.replace(woo_PATTERN, 'woo')\n",
    "reviews['review'] = reviews.review.str.replace(grill_PATTERN, 'grill')\n",
    "reviews['review'] = reviews.review.str.replace(worry_PATTERN, 'worry')\n",
    "reviews['review'] = reviews.review.str.replace(work_PATTERN, 'work')\n",
    "reviews['review'] = reviews.review.str.replace('saw', 'see')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=reviews['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add some stopwords to remove useless and redundant words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\",'!','?',\"*\",'actually','usually','really','mcds','''mcdonald's''',\n",
    "               'mcdonald','mcdonalds','''i'm''','''i've''','_ahhh_token_','time','number','say','alway',\n",
    "            'abbreviate','able','abour','underscore','Ã®_','uh','ew','gr','woo','thi','wa','ca', \"n't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = []\n",
    "for review in reviews:\n",
    "    words = review.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        word=singularize(word).lower()\n",
    "        if word.strip().lower() not in stopwords:\n",
    "            new_words.append(word)\n",
    "    cleaned_review = \" \".join(new_words)\n",
    "    cleaned_reviews.append(cleaned_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization with pos tag code is coming from gaurav5430's Github Gist**\n",
    "\n",
    "**The reason** I used lemmatization is we are doing a review text analysis and I hope the result of this analysis can more precisely reflect the true sentiment of the reviews. Using stemming may increase false positive rate and reduce precision since stemming  is trying to chop off the ends of words and will count some words which have different meanings into same root. Lemmatization will return the base or dictionary form of a word, which means it can match words more precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemma=[]\n",
    "for i in cleaned_reviews:\n",
    "    lemma_s=lemmatize_sentence(i)\n",
    "    df_lemma.append(lemma_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after lemmatizing, some words will change from non-stopword to stopwords -> abbreviated ->abbreviate\n",
    "# so I remove stopwords again after lemmatizing\n",
    "final_reviews = []\n",
    "for review in df_lemma:\n",
    "    words2 = review.split()\n",
    "    new_words2 = []\n",
    "    for word2 in words2:\n",
    "        word2=singularize(word2)\n",
    "        if word2.strip().lower() not in stopwords:\n",
    "            new_words2.append(word2)\n",
    "    final_reviews.append(new_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Used 2 for my 2-ngrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "collocation_finder = BigramCollocationFinder.from_documents(final_reviews)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generated TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2,2),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df=5, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= []\n",
    "for review in df_lemma:\n",
    "    words2 = review.split()\n",
    "    new_words2 = []\n",
    "    for word2 in words2:\n",
    "        word2=singularize(word2)\n",
    "        if word2.strip().lower() not in stopwords:\n",
    "            new_words2.append(word2)\n",
    "    final_review = \" \".join(new_words2)\n",
    "    corpus.append(final_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf =pd.DataFrame(tf_idf.sum(axis=1),columns=['tf_idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.sort_values(by=\"tf_idf\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20=tf_idf[:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top20.sort_values(by=\"tf_idf\",ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The features** my analysis showed that customers cited as reasons for a poor review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.barh(top20['index'],top20['tf_idf'])\n",
    "plt.yticks(rotation = 'horizontal',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for the food key word hot mustard and dip cone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[list(map(lambda line:bool(re.search(r'dip(?:ped)?\\scones?',line,flags=re.IGNORECASE)),reviews))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reviews[162])\n",
    "print(reviews[384])\n",
    "print(reviews[804])\n",
    "print(reviews[1067])\n",
    "print(reviews[1106])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[list(map(lambda line:bool(re.search(r'hot\\smustard?',line,flags=re.IGNORECASE)),reviews))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(reviews[367])\n",
    "print(reviews[493])\n",
    "print(reviews[613])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The most common issues identified from my analysis that generated customer dissatisfaction are:**\n",
    "\n",
    "* service is rude and slow; \n",
    "* customers need wait long time; \n",
    "* food is not clean and customers may have stomach ache after eating; \n",
    "* McDonalds' food make customers gain fat\n",
    "\n",
    "Recommendations: improve customer service and speed up the service; try to make clean and healthy food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Attribution (Feature Engineering and Regex Practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data:pd.DataFrame=pd.read_csv(\"truncated_catalog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender_info']=data.iloc[:,[1,2,3]].apply(lambda x:' '.join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter for only women's clothing items**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_womens_clothing']=list(map(lambda line:bool(re.search(\n",
    "    r'(wom(?:a|e)n|girl(?:friend)?s?|female|wifes?|wives|(?:grand)?daughters?|(?:grand)?monthers?|(?:grand)?mom(?:my)?s?|sisters?|nieces?|unisex)',\n",
    "                      line,flags=re.IGNORECASE)),data['gender_info']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('gender_info',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify its category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['category_info']=data.iloc[:,[1,2,3,5]].apply(lambda x:' '.join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if bool(re.search(r'(pumps?|stilettos?|heels|boot(?:ie)?s?wedges?|sandals?|flats?|platform|mary\\sjanes?|oxfords?|mules?|rainers?|loafers?|fantasys?|fantasies|shoes?|sneakers?)',\n",
    "                    data.loc[i,'category_info'],flags=re.IGNORECASE))==True:\n",
    "        data.loc[i,'category']='Shoe'\n",
    "    elif bool(re.search(r'((?:sweat)pants?|jeans?|skirts?|shorts?|leggings?|joggers?|bottoms?)',\n",
    "                    data.loc[i,'category_info'],flags=re.IGNORECASE))==True:\n",
    "        data.loc[i,'category']='Bottom'  \n",
    "    elif bool(re.search(r'(one(?:-|\\s)?piece|overalls?|(?:jump|play|body|swim)suits?|rompers?|onesies?|unitards?|dungarees?|dress(?:es)?)',\n",
    "                    data.loc[i,'category_info'],flags=re.IGNORECASE))==True:\n",
    "        data.loc[i,'category']='One Piece'  \n",
    "    elif bool(re.search(r'((?:hand)?bags?|satchels?|clutch(?:es)?|minaudieres?|wristlets?)',\n",
    "                    data.loc[i,'category_info'],flags=re.IGNORECASE))==True:\n",
    "        data.loc[i,'category']='Handbag'\n",
    "    elif bool(re.search(r'(scarfs?|scarves)',\n",
    "                    data.loc[i,'category_info'],flags=re.IGNORECASE))==True:\n",
    "        data.loc[i,'category']='Scarf' \n",
    "    else:\n",
    "        data.loc[i,'category']='null'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify its color**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['color']=data['category_info'].str.findall(r'(beige|black|blue|brown|burgundy|gold|gray|green|navy|neutral|orange|pinks|purple|red|silver|teal|white|yellow)',flags=re.IGNORECASE)\n",
    "data['colors']=data['color'].apply(lambda x:'Multi' if len(x)>1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('color',axis=1, inplace=True)\n",
    "data.drop('category_info',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
